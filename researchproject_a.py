# -*- coding: utf-8 -*-
"""ResearchProject_A.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EqD30LQPtNDSQ25t-GNmx54VC01JIC8M
"""

# step1_va_mapping.py
import pandas as pd
import numpy as np
import math
import os

# --------- Config ----------
IN_CSV  = "Objective1_Merged_CLEAN.csv"
OUT_CSV = "Objective1_with_VA.csv"
NRC_VAD_PATH = "NRC-VAD-Lexicon.txt"

# --------- GoEmotions → (valence, arousal) in [0,1] ----------
# ---------  VAD_MAP  ----------

VAD_MAP = {
    "admiration":     (0.969, 0.583),
    "amusement":      (0.929, 0.837),
    "anger":          (0.167, 0.865),
    "annoyance":      (0.167, 0.718),
    "approval":       (0.854, 0.460),
    "caring":         (0.635, 0.469),
    "confusion":      (0.255, 0.667),
    "curiosity":      (0.750, 0.755),
    "desire":         (0.896, 0.692),
    "disappointment": (0.115, 0.490),
    "disapproval":    (0.085, 0.551),
    "disgust":        (0.052, 0.775),
    "embarrassment":  (0.143, 0.685),
    "excitement":     (0.896, 0.684),
    "fear":           (0.073, 0.840),
    "gratitude":      (0.885, 0.441),
    "grief":          (0.070, 0.640),
    "joy":            (0.980, 0.824),
    "love":           (1.000, 0.519),
    "nervousness":    (0.163, 0.915),
    "optimism":       (0.949, 0.565),
    "pride":          (0.729, 0.634),
    "realization":    (0.554, 0.510),
    "relief":         (0.844, 0.278),
    "remorse":        (0.103, 0.673),
    "sadness":        (0.052, 0.288),
    "surprise":       (0.875, 0.875),  # valence of surprise varies; we set mid-high
    "neutral":        (0.469, 0.184),
}

# --------- Load data ----------
df = pd.read_csv(IN_CSV)
print(f"Loaded: {IN_CSV}  rows={len(df)}  cols={len(df.columns)}")

# Identify available GoEmotion probability columns safely
all_goem_cols = [c for c in df.columns if c.startswith("goem_")]
# Exclude non-probability helper columns
exclude = {"goem_top5_labels", "goem_top5_scores"}
prob_cols = [c for c in all_goem_cols if c not in exclude]

# Keep only those labels we have VAD for
labels_from_cols = [c.replace("goem_", "") for c in prob_cols]
labels_used = [lab for lab in labels_from_cols if lab in VAD_MAP]
used_cols = [f"goem_{lab}" for lab in labels_used]

if not used_cols:
    raise ValueError("No usable goem_* probability columns found. Check your CSV columns.")

print(f"Using {len(used_cols)} GoEmotions probability columns for V–A mapping.")

# --------- Compute weighted V–A per post ----------
V = []
A = []
I = []           # intensity (radius)
Theta_deg = []  # angle (degrees)
Quadrant = []   # pleasant/unpleasant × high/low arousal

for idx, row in df.iterrows():
    # weights and (v,a) pairs for present labels
    weights = []
    v_vals  = []
    a_vals  = []
    for lab in labels_used:
        p = float(row.get(f"goem_{lab}", 0.0))
        if p > 0:
            v, a = VAD_MAP[lab]
            weights.append(p)
            v_vals.append(v)
            a_vals.append(a)

    if len(weights) == 0 or sum(weights) == 0:
        v_post, a_post = np.nan, np.nan
    else:
        w = np.array(weights, dtype=float)
        v_arr = np.array(v_vals, dtype=float)
        a_arr = np.array(a_vals, dtype=float)
        w_sum = w.sum()
        v_post = float(np.dot(w, v_arr) / w_sum)
        a_post = float(np.dot(w, a_arr) / w_sum)

    V.append(v_post)
    A.append(a_post)

    # polar (center the circumplex at (0.5,0.5) → map to [-0.5, +0.5] then scale)
    if np.isnan(v_post) or np.isnan(a_post):
        I.append(np.nan)
        Theta_deg.append(np.nan)
        Quadrant.append("unknown")
    else:
        vx = v_post - 0.5
        ay = a_post - 0.5
        r = math.sqrt(vx*vx + ay*ay) * 2.0   # simple scaling so max radius ≈ 1
        theta = math.degrees(math.atan2(ay, vx))  # [-180, +180]
        I.append(r)
        Theta_deg.append(theta)

        # quadrant naming (you can tune thresholds later)
        val_side = "pleasant" if v_post >= 0.5 else "unpleasant"
        aro_side = "high_arousal" if a_post >= 0.5 else "low_arousal"
        Quadrant.append(f"{val_side}_{aro_side}")

df["vad_valence"] = V
df["vad_arousal"] = A
df["vad_intensity"] = I
df["vad_theta_deg"] = Theta_deg
df["vad_quadrant"] = Quadrant

# --------- Save ----------
df.to_csv(OUT_CSV, index=False)
print(f"Saved V–A augmented file → {OUT_CSV}")

# --------- Quick sanity summary ----------
valid = df.dropna(subset=["vad_valence", "vad_arousal"])
print("\nSanity check:")
print("Rows with valid V–A:", len(valid))
print(valid[["vad_valence","vad_arousal"]].describe())

print("\nQuadrant counts:")
print(valid["vad_quadrant"].value_counts())

# --------- (Optional) Quick scatter if you want to visualize locally ----------
import matplotlib.pyplot as plt
plt.figure(figsize=(6,6))
plt.scatter(valid["vad_valence"], valid["vad_arousal"], s=5, alpha=0.2)
plt.axvline(0.5, color='gray', linestyle='--')
plt.axhline(0.5, color='gray', linestyle='--')
plt.xlabel("Valence (0–1)")
plt.ylabel("Arousal (0–1)")
plt.title("Posts in Circumplex Space (Valence–Arousal)")
plt.tight_layout()
plt.show()

# Step 2: Balance severity classes before fine-tuning
import pandas as pd
from sklearn.utils import resample

# Load VA-augmented dataset
df = pd.read_csv("Objective1_with_VA.csv")

# Check original label distribution
print("Original Severity Distribution:")
print(df["Severity"].value_counts())

# --- Balancing ---
# Find the max class size
max_size = df["Severity"].value_counts().max()

# Perform oversampling for minority classes
balanced_df = pd.concat([
    resample(df[df["Severity"] == label],
             replace=True,
             n_samples=max_size,
             random_state=42)
    for label in df["Severity"].unique()
])

# Shuffle
balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)

print("\nBalanced Severity Distribution:")
print(balanced_df["Severity"].value_counts())

# Save
balanced_df.to_csv("Objective1_with_VA_balanced.csv", index=False)
print("\n✅ Saved balanced dataset as Objective1_with_VA_balanced.csv")

!nvidia-smi
!pip -q install -U transformers datasets accelerate evaluate sentencepiece scikit-learn matplotlib

import torch, platform, transformers
print("PyTorch:", torch.__version__, "| CUDA:", torch.cuda.is_available())
print("Transformers:", transformers.__version__, "| Python:", platform.python_version())
device = "cuda" if torch.cuda.is_available() else "cpu"

#Config

import os

CSV_PATH   = "/content/Objective1_with_VA_balanced.csv"
TEXT_COL   = "Text_With_EmojiDesc"
LABEL_COL  = "Severity"

MODEL_ID   = "microsoft/deberta-v3-base"
OUT_DIR    = "/content/severity-deberta-focal"

RANDOM_SEED = 42
NUM_EPOCHS  = 8
LR          = 2e-5
TRAIN_BS    = 16
EVAL_BS     = 16
MAX_LEN     = 384
GRAD_ACCUM  = 2
WARMUP      = 0.1
WEIGHT_DECAY= 0.01
PATIENCE    = 2          # early stopping

FOCAL_GAMMA = 2.0        # focal loss hyperparam
SAVE_DIR    = os.path.join(OUT_DIR, "best")
os.makedirs(SAVE_DIR, exist_ok=True)

!pip install -U --force-reinstall scikit-learn

#Load adata and split

import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv(CSV_PATH)
df = df[[TEXT_COL, LABEL_COL]].dropna().reset_index(drop=True)
df[TEXT_COL] = df[TEXT_COL].astype(str).str.strip()
df = df[df[TEXT_COL].str.len() > 0].reset_index(drop=True)

label_list = ["mild_distress","moderate_distress","severe_crisis"]
label2id = {l:i for i,l in enumerate(label_list)}
id2label = {i:l for l,i in label2id.items()}
df["labels"] = df[LABEL_COL].map(label2id).astype(int)

# 80/10/10 split
train_df, temp_df = train_test_split(
    df, test_size=0.20, random_state=RANDOM_SEED, stratify=df["labels"]
)
val_df, test_df = train_test_split(
    temp_df, test_size=0.50, random_state=RANDOM_SEED, stratify=temp_df["labels"]
)

print("Train/Val/Test sizes:", len(train_df), len(val_df), len(test_df))
print("Train distribution:\n", train_df["labels"].value_counts(normalize=True))

from datasets import Dataset, DatasetDict
from transformers import AutoTokenizer, DataCollatorWithPadding

tok = AutoTokenizer.from_pretrained(MODEL_ID)

train_ds = Dataset.from_pandas(train_df[[TEXT_COL,"labels"]], preserve_index=False)
val_ds   = Dataset.from_pandas(val_df[[TEXT_COL,"labels"]],   preserve_index=False)
test_ds  = Dataset.from_pandas(test_df[[TEXT_COL,"labels"]],  preserve_index=False)
ds = DatasetDict({"train": train_ds, "validation": val_ds, "test": test_ds})

def tokenize(batch):
    return tok(batch[TEXT_COL], truncation=True, max_length=MAX_LEN)
ds = ds.map(tokenize, batched=True, remove_columns=[TEXT_COL])

# dynamic padding saves memory
collator = DataCollatorWithPadding(tok)

#model + focal loss trainer

import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
from transformers import (
    AutoModelForSequenceClassification,
    TrainingArguments, Trainer, EarlyStoppingCallback
)
import torch.nn.functional as F
import torch

model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_ID, num_labels=3, id2label=id2label, label2id=label2id
)

# Focal loss module
class FocalLoss(torch.nn.Module):
    def __init__(self, gamma=2.0):
        super().__init__()
        self.gamma = gamma
    def forward(self, logits, target):
        ce = F.cross_entropy(logits, target, reduction="none")
        pt = torch.exp(-ce)
        loss = ((1 - pt) ** self.gamma) * ce
        return loss.mean()

focal = FocalLoss(gamma=FOCAL_GAMMA)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average="macro", zero_division=0)
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "precision": prec, "recall": rec, "f1": f1}

# Subclass Trainer to inject focal loss (works with current Transformers)
class FocalTrainer(Trainer):
    def __init__(self, *args, focal_loss, **kwargs):
        super().__init__(*args, **kwargs)
        self.focal = focal_loss

    # NOTE the **kwargs to swallow num_items_in_batch etc.
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs.get("labels")
        outputs = model(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
        )
        logits = outputs.logits
        loss = self.focal.to(logits.device)(logits, labels)
        return (loss, outputs) if return_outputs else loss

!pip install -U transformers datasets accelerate evaluate
!pip install sentencepiece

import torch
use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8
print("GPU:", torch.cuda.get_device_name(0))
print("Supports bf16:", use_bf16)

!pip install -U transformers datasets accelerate evaluate
!pip install sentencepiece

import torch
use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8
print("GPU:", torch.cuda.get_device_name(0))
print("Supports bf16:", use_bf16)

#Training
use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8  # A100/L4
args = TrainingArguments(
    output_dir=OUT_DIR,
    seed=RANDOM_SEED,
    learning_rate=LR,
    per_device_train_batch_size=TRAIN_BS,
    per_device_eval_batch_size=EVAL_BS,
    gradient_accumulation_steps=GRAD_ACCUM,
    num_train_epochs=NUM_EPOCHS,
    weight_decay=WEIGHT_DECAY,
    warmup_ratio=WARMUP,
    eval_strategy="epoch",       # correct for recent transformers
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    greater_is_better=True,
    logging_steps=50,
    report_to="none",
    bf16=use_bf16,               # mixed precision on A100/L4
)

trainer = FocalTrainer(
    model=model,
    args=args,
    train_dataset=ds["train"],
    eval_dataset=ds["validation"],
    tokenizer=tok,   # FutureWarning is harmless; you can ignore it
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE)],
    focal_loss=focal,
)

trainer.train()

#evaluate on test

# Test metrics
metrics = trainer.evaluate(ds["test"])
print("\nTest metrics:", metrics)

# Per-row predictions
pred_logits = trainer.predict(ds["test"]).predictions
y_pred = pred_logits.argmax(axis=-1)
y_true = test_df["labels"].to_numpy()

# Report + confusion matrix
print("\nClassification report (macro):\n")
print(classification_report(y_true, y_pred, target_names=label_list, digits=3))

cm = confusion_matrix(y_true, y_pred)
pd.DataFrame(cm, index=label_list, columns=label_list).to_csv(os.path.join(OUT_DIR,"confusion_matrix_test.csv"))

# Save best model + tokenizer
trainer.model.save_pretrained(SAVE_DIR)
tok.save_pretrained(SAVE_DIR)
print("Saved best model to:", SAVE_DIR)

# Save predictions for audit
dump = test_df.copy()
dump["pred_id"] = y_pred
dump["pred"]    = dump["pred_id"].map({i:l for i,l in enumerate(label_list)})
dump.to_csv(os.path.join(OUT_DIR,"test_predictions.csv"), index=False)
print("Saved predictions to:", os.path.join(OUT_DIR,"test_predictions.csv"))

#Load and merge predictions with V-A

import os, sys, math, random
import numpy as np
import pandas as pd


BALANCED_CSV = "/content/Objective1_with_VA_balanced.csv"
PRED_CSV     = "/content/severity-deberta-focal/test_predictions.csv"
OUT_DIR      = "/content/va_severity_eda"
os.makedirs(OUT_DIR, exist_ok=True)

# Load data
df_va = pd.read_csv(BALANCED_CSV)
df_pred = pd.read_csv(PRED_CSV)

print("Balanced VA file columns:", list(df_va.columns)[:12], "...")
print("Predictions file columns:", list(df_pred.columns)[:12], "...")

# Normalize column names we rely on
# Try to find TEXT and ID columns
TEXT_CANDIDATES = ["Text_With_EmojiDesc", "Full_Text", "Preprocessed_Text", "text", "Text"]
ID_CANDIDATES   = ["Post_ID", "post_id", "id", "PostId"]

def pick_col(cols, candidates):
    for c in candidates:
        if c in cols:
            return c
    return None

text_col_va   = pick_col(df_va.columns, TEXT_CANDIDATES)
text_col_pred = pick_col(df_pred.columns, TEXT_CANDIDATES)
id_col_va     = pick_col(df_va.columns, ID_CANDIDATES)
id_col_pred   = pick_col(df_pred.columns, ID_CANDIDATES)

# Make sure severity/pred columns exist
# Test dump usually has true label as either 'labels' or 'Severity', and prediction as 'pred' or 'pred_label'
if "Severity" not in df_va.columns:
    # if we lost it during balancing, try to map from any label column
    if "labels" in df_va.columns:
        mapping = {0:"mild_distress",1:"moderate_distress",2:"severe_crisis"}
        df_va["Severity"] = df_va["labels"].map(mapping)

pred_col = None
for c in ["pred", "pred_label", "pred_class", "pred_severity"]:
    if c in df_pred.columns:
        pred_col = c
        break
if pred_col is None and "pred_label_id" in df_pred.columns:
    mapping = {0:"mild_distress",1:"moderate_distress",2:"severe_crisis"}
    df_pred["pred"] = df_pred["pred_label_id"].map(mapping)
    pred_col = "pred"

# Merge logic: prefer Post_ID if present; else use text
if id_col_va and id_col_pred and id_col_va in df_va and id_col_pred in df_pred:
    key = ("ID", id_col_va, id_col_pred)
    merged = df_pred.merge(df_va, left_on=id_col_pred, right_on=id_col_va, how="left")
else:
    # fall back to text join (exact match)
    if text_col_va is None or text_col_pred is None:
        raise ValueError("Could not find a common join key. Add/rename Post_ID or Text column in your files.")
    key = ("TEXT", text_col_va, text_col_pred)
    merged = df_pred.merge(df_va, left_on=text_col_pred, right_on=text_col_va, how="left")

needed_cols = ["vad_valence","vad_arousal"]
missing = [c for c in needed_cols if c not in merged.columns]
if missing:
    raise ValueError(f"Missing columns after merge: {missing}. Make sure your balanced CSV has vad_valence & vad_arousal.")

# After merging:
print(f"\nJoined on {key[0]} → pred[{key[2]}] ↔ va[{key[1]}]")
print("Merged shape:", merged.shape)

# Inspect to find actual column names
print("Merged columns:", merged.columns.tolist())

# Use the correct severity column (either 'Severity_x' or 'Severity_y')
severity_col = [c for c in merged.columns if "Severity" in c][0]
print(f"Using severity column: {severity_col}")

# Show first few rows for verification
print(merged[[pred_col, severity_col, "vad_valence", "vad_arousal"]].head())

# Clean rows with NA V–A or predictions
merged = merged.dropna(subset=[pred_col, "vad_valence", "vad_arousal"])
merged.to_csv("Merged_with_VA_and_Predictions.csv", index=False)
print("\n✅ Saved merged file as Merged_with_VA_and_Predictions.csv")

# --- Cell 2: Summary tables (means, CIs) and save CSV ---

import pandas as pd
import numpy as np
from scipy import stats

# Load merged dataset from previous step
merged = pd.read_csv("Merged_with_VA_and_Predictions.csv")

# Remove any missing V-A values (if any)
merged = merged.dropna(subset=["vad_valence", "vad_arousal"])

# Group by predicted severity
g = merged.groupby("pred")

# Compute mean and 95% CI for each group
summary = g[["vad_valence", "vad_arousal"]].mean()
summary.columns = ["mean_valence", "mean_arousal"]

def ci95(series):
    m = series.mean()
    se = stats.sem(series)
    h = se * stats.t.ppf((1 + 0.95) / 2, len(series) - 1)
    return pd.Series({"ci_low": m - h, "ci_high": m + h})

ci_val = g["vad_valence"].apply(ci95)
ci_aro = g["vad_arousal"].apply(ci95)

# Merge means + CIs
summary = (
    summary.join(ci_val.add_prefix("valence_"))
            .join(ci_aro.add_prefix("arousal_"))
            .reset_index()
)

print("\nSummary by predicted severity (valence/arousal with 95% CI):")
print(summary.round(3))

# Save to CSV
summary.to_csv("Objective1_with_VA_summary.csv", index=False)
print(" Saved summary to Objective1_with_VA_summary.csv")

import matplotlib.pyplot as plt
import matplotlib.ticker as mtick

pred_col = "pred" if "pred" in merged.columns else "pred_label"
classes = ["mild_distress","moderate_distress","severe_crisis"]
colors  = {"mild_distress":"tab:green","moderate_distress":"tab:orange","severe_crisis":"tab:red"}

# 1) Scatter (downsample to avoid overplotting)
plot_df = merged.sample(n=min(5000, len(merged)), random_state=42)
plt.figure(figsize=(7.5,6.5))
for c in classes:
    d = plot_df[plot_df[pred_col]==c]
    plt.scatter(d["vad_valence"], d["vad_arousal"], s=10, alpha=0.35, label=c, c=colors[c])
plt.axvline(0.5, ls="--", c="gray", lw=1)
plt.axhline(0.6, ls="--", c="gray", lw=1)
plt.xlim(0,1); plt.ylim(0,1)
plt.xlabel("Valence (0–1)"); plt.ylabel("Arousal (0–1)")
plt.title("Valence–Arousal by Predicted Severity (sampled)")
plt.legend(frameon=False)
plt.tight_layout()
plt.savefig(os.path.join(OUT_DIR, "scatter_va_by_pred.png"), dpi=200)
plt.show()

# 2) Boxplots (Valence and Arousal)
def boxplot_metric(metric, title, filename):
    order = classes
    data = [merged.loc[merged[pred_col]==c, metric].values for c in order]
    plt.figure(figsize=(7,5))
    bp = plt.boxplot(data, labels=order, patch_artist=True)
    for patch, c in zip(bp['boxes'], order):
        patch.set_facecolor(colors[c]); patch.set_alpha(0.5)
    plt.ylabel(metric.replace("_"," ").title())
    plt.title(title)
    plt.ylim(0,1)
    plt.tight_layout()
    plt.savefig(os.path.join(OUT_DIR, filename), dpi=200)
    plt.show()

boxplot_metric("vad_valence", "Valence by Predicted Severity", "box_valence_by_pred.png")
boxplot_metric("vad_arousal", "Arousal by Predicted Severity", "box_arousal_by_pred.png")

# 3) Quadrant distribution per severity (stacked bars)
ct = pd.crosstab(merged[pred_col], merged["vad_quadrant"]).reindex(classes)
ct_pct = (ct.T / ct.sum(axis=1)).T  # row-normalized
ax = ct_pct.plot(kind="bar", stacked=True, color=["#d62728","#ff9896","#1f77b4","#aec7e8"], figsize=(8,5))
ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))
plt.title("V–A Quadrants by Predicted Severity")
plt.ylabel("Share of posts")
plt.xlabel("")
plt.legend(title="Quadrant", bbox_to_anchor=(1.02,1), loc="upper left", frameon=False)
plt.tight_layout()
plt.savefig(os.path.join(OUT_DIR, "stacked_quadrants_by_pred.png"), dpi=200)
plt.show()

ct.to_csv(os.path.join(OUT_DIR, "quadrant_counts_by_pred.csv"))
ct_pct.to_csv(os.path.join(OUT_DIR, "quadrant_share_by_pred.csv"))
print("Saved quadrant tables.")

from scipy import stats

pred_col = "pred" if "pred" in merged.columns else "pred_label"
label_map = {"mild_distress":0, "moderate_distress":1, "severe_crisis":2}
merged["pred_id"] = merged[pred_col].map(label_map).astype(int)

def anova_and_kruskal(metric):
    groups = [merged.loc[merged["pred_id"]==i, metric].dropna().values for i in [0,1,2]]
    # ANOVA
    f, p = stats.f_oneway(*groups)
    # eta-squared (effect size)
    all_x = merged[metric].dropna().values
    grand_mean = all_x.mean()
    ss_between = sum([len(g)*(g.mean()-grand_mean)**2 for g in groups])
    ss_total   = ((all_x - grand_mean)**2).sum()
    eta_sq = ss_between/ss_total if ss_total>0 else np.nan
    # Kruskal (non-parametric)
    H, p_kw = stats.kruskal(*groups)
    return {"metric":metric,"anova_F":f,"anova_p":p,"eta_sq":eta_sq,"kruskal_H":H,"kruskal_p":p_kw}

res_val = anova_and_kruskal("vad_valence")
res_aro = anova_and_kruskal("vad_arousal")

# Spearman correlation with ordinal severity
rho_v, p_v = stats.spearmanr(merged["pred_id"], merged["vad_valence"])
rho_a, p_a = stats.spearmanr(merged["pred_id"], merged["vad_arousal"])

stats_df = pd.DataFrame([
    res_val, res_aro,
    {"metric":"spearman_valence","rho":rho_v,"p":p_v},
    {"metric":"spearman_arousal","rho":rho_a,"p":p_a},
])
stats_df.to_csv(os.path.join(OUT_DIR, "va_stats_summary.csv"), index=False)
stats_df

# --- Step 5A (robust): Load & merge Emotion + Severity Data ---

import pandas as pd

# Files
EMO_FILE = "Cleaned_Step5_GoEmotionTagged.csv"
# Use the merged file you saved in Step 4 (has Post_ID, Full_Text, pred, vad_valence, vad_arousal…)
PRED_FILE = "Merged_with_VA_and_Predictions.csv"

emo = pd.read_csv(EMO_FILE)
pred = pd.read_csv(PRED_FILE)

print("Emotion file columns (first 12):", emo.columns[:12].tolist())
print("Pred/V-A file columns (first 12):", pred.columns[:12].tolist())

# Prefer Post_ID if in both; else fall back to Full_Text
join_key = None
if "Post_ID" in emo.columns and "Post_ID" in pred.columns:
    join_key = "Post_ID"
elif "Full_Text" in emo.columns and "Full_Text" in pred.columns:
    join_key = "Full_Text"
else:
    raise ValueError(
        "No common join key found. Ensure PRED_FILE has either Post_ID or Full_Text "
        "and that EMO_FILE has the same column."
    )

# Keep only what we need from pred file (drop dups on the key)
keep_cols = [join_key, "pred"]  # 'pred' = predicted severity (mild/moderate/severe)
if "vad_valence" in pred.columns and "vad_arousal" in pred.columns:
    keep_cols += ["vad_valence", "vad_arousal"]
pred_small = pred[keep_cols].drop_duplicates(subset=[join_key]).copy()

# Merge
merged = emo.merge(pred_small, on=join_key, how="inner")
merged = merged.rename(columns={"pred": "Predicted_Severity"})
print(f"\nJoined on '{join_key}'. Merged shape: {merged.shape}")
print(merged[[join_key, "Predicted_Severity"] + [c for c in ["vad_valence","vad_arousal"] if c in merged.columns]].head())

# Save the merged table for later steps
merged.to_csv("Emotions_with_Predictions_MERGED.csv", index=False)
print("✅ Saved: Emotions_with_Predictions_MERGED.csv")